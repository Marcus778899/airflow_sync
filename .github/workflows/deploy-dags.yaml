name: Deploy Airflow DAGs

on:
  push:
    branches:
      - main
    paths:
      - 'dags/**'
  workflow_dispatch: {}

jobs:
  deploy:
    runs-on: self-hosted
    defaults:
      run:
        shell: bash

    env:
      AIRFLOW_DAG_DIR: ${{ secrets.AIRFLOW_DAG_DIR }}
      AIRFLOW_RELEASES_DIR: ${{ secrets.AIRFLOW_RELEASES_DIR }}
      SSH_HOST: ${{ secrets.SSH_HOST }}
      SSH_USER: ${{ secrets.SSH_USER }}
      SSH_PASSWORD: ${{ secrets.SSH_PASSWORD }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # 1. upload DAGs to new release folder
      - name: Create release & upload DAGs
        run: |
          TS=$(date -u +%Y%m%d_%H%M%S)
          echo "TS=${TS}" >> $GITHUB_ENV
          REMOTE_RELEASE="${AIRFLOW_RELEASES_DIR}/${TS}"
          echo "Uploading DAGs to ${REMOTE_RELEASE} ..."

          ssh ${SSH_USER}@${SSH_HOST} "mkdir -p ${REMOTE_RELEASE}"
          ls -al ${REMOTE_RELEASE}
          rsync -avz --delete dags/ ${SSH_USER}@${SSH_HOST}:${REMOTE_RELEASE}/
          echo "Uploaded to ${REMOTE_RELEASE}"

      # 2. check uploaded files
      - name: Validate uploaded DAGs (pre-switch)
        run: |
          ssh ${SSH_USER}@${SSH_HOST} "set -e
            SUBDIR=/opt/airflow/releases/${TS}
            echo "Checking DAG files in: ${SUBDIR}"
            if [ ! -d ${SUBDIR} ]; then
              echo 'Release folder not found!'
              exit 1
            fi
            echo Folder exists. Listing contents:
            ls -al ${SUBDIR}
          "

      # 3. switch symlink and refresh DAGs
      - name: Switch symlink atomically
        run: |
          ssh ${SSH_USER}@${SSH_HOST} "set -e
            CURRENT_LINK=${AIRFLOW_DAG_DIR}
            NEW_RELEASE=${AIRFLOW_RELEASES_DIR}/${TS}

            echo "Switching symlink: ${CURRENT_LINK} -> ${NEW_RELEASE}
            if [ ! -d ${NEW_RELEASE} ]; then
              echo 'Release not found: '${NEW_RELEASE}
              exit 1
            fi

            ln -sfn ${NEW_RELEASE} ${CURRENT_LINK}
            find ${CURRENT_LINK} -type f -name '*.py' -exec touch {} +
            echo Symlink updated successfully

            CID=$(docker ps --filter 'name=airflow-apiserver' --format '{{.ID}}' | head -n 1)
            if [ -z ${CID} ]; then
              echo 'No Airflow container found'
              exit 1
            fi
            echo Found container ${CID}

            echo Refreshing DAGs...
            docker exec ${CID} airflow dags list >/dev/null 2>&1 &&
              echo Airflow DAGs loaded successfully ||
              (echo Airflow DAGs load failed && exit 1)
          "

      # 4. health check
      - name: Airflow service health check
        run: |
          ssh ${SSH_USER}@${SSH_HOST} "set -e
            CID=$(docker ps --filter 'name=airflow-apiserver' --format '{{.ID}}' | head -n 1)
            if [ -z ${CID} ]; then
              echo 'Airflow container not found!'
              exit 1
            fi
            echo Found container ${CID}
            docker exec ${CID} airflow dags list >/dev/null 2>&1 &&
              echo Airflow DAGs loaded successfully ||
              (echo Airflow DAGs load failed && exit 1)
          "

      # 5. cleanup old versions
      - name: Cleanup old releases (keep last 5)
        run: |
          ssh ${SSH_USER}@${SSH_HOST} "set -e
            cd ${AIRFLOW_RELEASES_DIR}
            ls -1t | tail -n +6 | xargs -r rm -rf
            echo Old releases cleaned
          "